{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Emotion classification\n",
    "The problem type is supervised multiclass classification and the target is the emotion, with the different classes being ('sadness', 'anger', 'love', 'surprise', 'fear', 'joy').  \n",
    "To do this we're going to apply transfer learning by using a model pre-trained specifically on this task  \n",
    "The model is provided by hugging face\n",
    "https://huggingface.co/mrm8488/t5-base-finetuned-emotion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lujai\\anaconda3\\envs\\capstone-proj\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1177: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad> sadness'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hugging face tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "# load the model which is already trained on emotion dataset\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "# function that takes input and returns emotion\n",
    "def get_emotion(text):\n",
    "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=input_ids,\n",
    "               max_length=2)\n",
    "  \n",
    "  dec = [tokenizer.decode(ids) for ids in output]\n",
    "  label = dec[0]\n",
    "  return label\n",
    "  \n",
    "#get_emotion(\"i feel as if i havent blogged in ages are at least truly blogged i am doing an update cute\") # Output: 'joy'\n",
    " \n",
    "get_emotion(\"i have a feeling i kinda lost my best friend\") # Output: 'sadness'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Emoji emotion classification\n",
    "In this part, we're going to use the previous trained model to help us predict the emotions of the emojis. The feature is the emoji name, e.g., FACE WITH TEARS OF JOY. and the target variable is the emotion.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Read the tweet-emoji dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem with this dataset is that it saves the names of the emojis not the emojis themselves # we will solve this by merging it with another dataset\n",
    "tweets = pd.read_csv(\"../../../Desktop/tweets_emojis.csv\") #problem: some unicode names here don't match unicode names in emoji dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete uneeded column\n",
    "tweets.drop('Unnamed: 0', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Unicode name'] = tweets['emoji']\n",
    "# remove all characters that are not letters or numbers # save it in new column called unicode name to help with merge later\n",
    "tweets['Unicode name'] = tweets['Unicode name'].str.replace('_', ' ')\n",
    "# convert to upper case\n",
    "tweets['Unicode name']= tweets['Unicode name'].apply(lambda names: names.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop old column\n",
    "tweets.drop('emoji', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 FACE WITH TEARS OF JOY\n",
       "1                 FACE WITH TEARS OF JOY\n",
       "2                              THUMBS UP\n",
       "3                 FACE WITH TEARS OF JOY\n",
       "4                         CLAPPING HANDS\n",
       "                       ...              \n",
       "1320030                        MALE SIGN\n",
       "1320031    BACKHAND INDEX POINTING RIGHT\n",
       "1320032                     FLUSHED FACE\n",
       "1320033                 PERSON SHRUGGING\n",
       "1320034                    RAISING HANDS\n",
       "Name: Unicode name, Length: 1320035, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Unicode name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Unicode name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Idk who taught my baby this BS  Ô∏è  IGmeetthesa...</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thats me in every lesson</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are MANY of you  üá∫ üá∏  üá∫ üá∏  üáÆ üá±</td>\n",
       "      <td>THUMBS UP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Partner strategy LLRC  Urban naxal theories ar...</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy Birthday More blessings Matsatsi  üèΩ  Hop...</td>\n",
       "      <td>CLAPPING HANDS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            Unicode name\n",
       "0  Idk who taught my baby this BS  Ô∏è  IGmeetthesa...  FACE WITH TEARS OF JOY\n",
       "1                          Thats me in every lesson   FACE WITH TEARS OF JOY\n",
       "2               There are MANY of you  üá∫ üá∏  üá∫ üá∏  üáÆ üá±               THUMBS UP\n",
       "3  Partner strategy LLRC  Urban naxal theories ar...  FACE WITH TEARS OF JOY\n",
       "4  Happy Birthday More blessings Matsatsi  üèΩ  Hop...          CLAPPING HANDS"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FACE WITH TEARS OF JOY            210309\n",
       "RED HEART                         103172\n",
       "LOUDLY CRYING FACE                 82659\n",
       "SMILING FACE WITH HEART-EYES       67817\n",
       "FIRE                               51030\n",
       "FEMALE SIGN                        50941\n",
       "MALE SIGN                          34149\n",
       "FOLDED HANDS                       32273\n",
       "WEARY FACE                         30489\n",
       "TWO HEARTS                         30049\n",
       "PERSON SHRUGGING                   29903\n",
       "SMILING FACE WITH SMILING EYES     28519\n",
       "RAISING HANDS                      26444\n",
       "THINKING FACE                      26362\n",
       "PERSON FACEPALMING                 24303\n",
       "HUNDRED POINTS                     23743\n",
       "SPARKLES                           23532\n",
       "FACE WITH ROLLING EYES             21917\n",
       "CLAPPING HANDS                     21462\n",
       "ROLLING ON THE FLOOR LAUGHING      21335\n",
       "FACE BLOWING A KISS                20878\n",
       "EYES                               20824\n",
       "THUMBS UP                          19456\n",
       "BACKHAND INDEX POINTING RIGHT      17971\n",
       "FLEXED BICEPS                      16634\n",
       "PURPLE HEART                       16457\n",
       "PARTY POPPER                       16259\n",
       "WINKING FACE                       16075\n",
       "BLUE HEART                         15941\n",
       "SMILING FACE WITH SUNGLASSES       15322\n",
       "OK HAND                            14990\n",
       "SPARKLING HEART                    14614\n",
       "BEAMING FACE WITH SMILING EYES     14568\n",
       "HEART SUIT                         14260\n",
       "DOUBLE EXCLAMATION MARK            13951\n",
       "POLICE CAR LIGHT                   12552\n",
       "SMILING FACE                       12338\n",
       "BACKHAND INDEX POINTING DOWN       12075\n",
       "SKULL                              11976\n",
       "CRYING FACE                        11265\n",
       "YELLOW HEART                       10100\n",
       "RIGHT ARROW                         9094\n",
       "SPEAKING HEAD                       9031\n",
       "FLUSHED FACE                        8449\n",
       "COLLISION                           8363\n",
       "WHITE HEAVY CHECK MARK              7192\n",
       "TROPHY                              6892\n",
       "GLOWING STAR                        6189\n",
       "HEAVY CHECK MARK                    5911\n",
       "Name: Unicode name, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Unicode name'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Read the emoji-unicode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Unicode codepoint</th>\n",
       "      <th>Occurrences</th>\n",
       "      <th>Position</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Unicode name</th>\n",
       "      <th>Unicode block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üòÇ</td>\n",
       "      <td>0x1f602</td>\n",
       "      <td>14622</td>\n",
       "      <td>0.805101</td>\n",
       "      <td>3614</td>\n",
       "      <td>4163</td>\n",
       "      <td>6845</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>Emoticons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚ù§</td>\n",
       "      <td>0x2764</td>\n",
       "      <td>8050</td>\n",
       "      <td>0.746943</td>\n",
       "      <td>355</td>\n",
       "      <td>1334</td>\n",
       "      <td>6361</td>\n",
       "      <td>HEAVY BLACK HEART</td>\n",
       "      <td>Dingbats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‚ô•</td>\n",
       "      <td>0x2665</td>\n",
       "      <td>7144</td>\n",
       "      <td>0.753806</td>\n",
       "      <td>252</td>\n",
       "      <td>1942</td>\n",
       "      <td>4950</td>\n",
       "      <td>BLACK HEART SUIT</td>\n",
       "      <td>Miscellaneous Symbols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üòç</td>\n",
       "      <td>0x1f60d</td>\n",
       "      <td>6359</td>\n",
       "      <td>0.765292</td>\n",
       "      <td>329</td>\n",
       "      <td>1390</td>\n",
       "      <td>4640</td>\n",
       "      <td>SMILING FACE WITH HEART-SHAPED EYES</td>\n",
       "      <td>Emoticons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üò≠</td>\n",
       "      <td>0x1f62d</td>\n",
       "      <td>5526</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>2412</td>\n",
       "      <td>1218</td>\n",
       "      <td>1896</td>\n",
       "      <td>LOUDLY CRYING FACE</td>\n",
       "      <td>Emoticons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emoji Unicode codepoint  Occurrences  Position  Negative  Neutral  Positive  \\\n",
       "0     üòÇ           0x1f602        14622  0.805101      3614     4163      6845   \n",
       "1     ‚ù§            0x2764         8050  0.746943       355     1334      6361   \n",
       "2     ‚ô•            0x2665         7144  0.753806       252     1942      4950   \n",
       "3     üòç           0x1f60d         6359  0.765292       329     1390      4640   \n",
       "4     üò≠           0x1f62d         5526  0.803352      2412     1218      1896   \n",
       "\n",
       "                          Unicode name          Unicode block  \n",
       "0               FACE WITH TEARS OF JOY              Emoticons  \n",
       "1                    HEAVY BLACK HEART               Dingbats  \n",
       "2                     BLACK HEART SUIT  Miscellaneous Symbols  \n",
       "3  SMILING FACE WITH HEART-SHAPED EYES              Emoticons  \n",
       "4                   LOUDLY CRYING FACE              Emoticons  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this dataset to get the emojis\n",
    "emojis = pd.read_csv(\"../data/emojis/Emoji_Sentiment_Data_v1.0.csv\")\n",
    "emojis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we dont need all features, only save emoji and name\n",
    "emojis = emojis[['Emoji', 'Unicode name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Unicode name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üòÇ</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚ù§</td>\n",
       "      <td>HEAVY BLACK HEART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‚ô•</td>\n",
       "      <td>BLACK HEART SUIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üòç</td>\n",
       "      <td>SMILING FACE WITH HEART-SHAPED EYES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üò≠</td>\n",
       "      <td>LOUDLY CRYING FACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emoji                         Unicode name\n",
       "0     üòÇ               FACE WITH TEARS OF JOY\n",
       "1     ‚ù§                    HEAVY BLACK HEART\n",
       "2     ‚ô•                     BLACK HEART SUIT\n",
       "3     üòç  SMILING FACE WITH HEART-SHAPED EYES\n",
       "4     üò≠                   LOUDLY CRYING FACE"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after\n",
    "emojis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Unicode name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üòÇ</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>Idk who taught my baby this BS  Ô∏è  IGmeetthesa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üòÇ</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>Thats me in every lesson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üòÇ</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>Partner strategy LLRC  Urban naxal theories ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üòÇ</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>Dont play with me  üèæ ‚Äç  Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üòÇ</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>To the goofiest boy ever who apparently looks ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emoji            Unicode name  \\\n",
       "0     üòÇ  FACE WITH TEARS OF JOY   \n",
       "1     üòÇ  FACE WITH TEARS OF JOY   \n",
       "2     üòÇ  FACE WITH TEARS OF JOY   \n",
       "3     üòÇ  FACE WITH TEARS OF JOY   \n",
       "4     üòÇ  FACE WITH TEARS OF JOY   \n",
       "\n",
       "                                                text  \n",
       "0  Idk who taught my baby this BS  Ô∏è  IGmeetthesa...  \n",
       "1                          Thats me in every lesson   \n",
       "2  Partner strategy LLRC  Urban naxal theories ar...  \n",
       "3                         Dont play with me  üèæ ‚Äç  Ô∏è   \n",
       "4  To the goofiest boy ever who apparently looks ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the two dataset on the unicode name to get a tweet - emoji dataset\n",
    "tweets_emojis = emojis.merge(tweets)\n",
    "tweets_emojis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(741777, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_emojis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FACE WITH TEARS OF JOY            210309\n",
       "LOUDLY CRYING FACE                 82659\n",
       "FIRE                               51030\n",
       "FEMALE SIGN                        50941\n",
       "MALE SIGN                          34149\n",
       "WEARY FACE                         30489\n",
       "TWO HEARTS                         30049\n",
       "SMILING FACE WITH SMILING EYES     28519\n",
       "SPARKLES                           23532\n",
       "EYES                               20824\n",
       "FLEXED BICEPS                      16634\n",
       "PURPLE HEART                       16457\n",
       "PARTY POPPER                       16259\n",
       "WINKING FACE                       16075\n",
       "BLUE HEART                         15941\n",
       "SMILING FACE WITH SUNGLASSES       15322\n",
       "SPARKLING HEART                    14614\n",
       "SKULL                              11976\n",
       "CRYING FACE                        11265\n",
       "YELLOW HEART                       10100\n",
       "FLUSHED FACE                        8449\n",
       "WHITE HEAVY CHECK MARK              7192\n",
       "TROPHY                              6892\n",
       "GLOWING STAR                        6189\n",
       "HEAVY CHECK MARK                    5911\n",
       "Name: Unicode name, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the distribution of emojis\n",
    "tweets_emojis['Unicode name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_emojis.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this dataframe is too large, we need a way to query info more efficiently so we create a dictionary\n",
    "emoji_dict = {k: v for k, v in tweets_emojis.groupby('Unicode name')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLUE HEART':        Emoji Unicode name                                               text\n",
       " 462064     üíô   BLUE HEART                                  OBF Lets go AMVT \n",
       " 462065     üíô   BLUE HEART  BEAUTIFUL  Ô∏è  Ô∏è  Ô∏è RaIna  Ô∏è  Unforgettable Mem...\n",
       " 462066     üíô   BLUE HEART                        So much love for youman  Ô∏è \n",
       " 462067     üíô   BLUE HEART  Idc what my man have or dont have Ima ride w h...\n",
       " 462068     üíô   BLUE HEART  This is JUST the beginning Number 9 on iTunes ...\n",
       " ...      ...          ...                                                ...\n",
       " 478000     üíô   BLUE HEART  Thanasi with a convincing win over Alexander B...\n",
       " 478001     üíô   BLUE HEART                      I love you so much hermanita \n",
       " 478002     üíô   BLUE HEART                    Congrats to the whole family  üèº\n",
       " 478003     üíô   BLUE HEART                          All My Heart also played \n",
       " 478004     üíô   BLUE HEART    ITS OFFICIAL Arnold Hall Room 153 with my girl \n",
       " \n",
       " [15941 rows x 3 columns],\n",
       " 'CRYING FACE':        Emoji Unicode name                                               text\n",
       " 525308     üò¢  CRYING FACE                        Why you do this to me kuya \n",
       " 525309     üò¢  CRYING FACE  Please Dont Scroll Past Without  Retweeting My...\n",
       " 525310     üò¢  CRYING FACE  I miss „Ö†„Ö†„Ö† so much Im just semi hiatus and the...\n",
       " 525311     üò¢  CRYING FACE  Dancing to no limit got me thinking of  miss y...\n",
       " 525312     üò¢  CRYING FACE   I really thought that last question with Adam...\n",
       " ...      ...          ...                                                ...\n",
       " 536568     üò¢  CRYING FACE  Awwwh the CEO just gave a whole thank you spee...\n",
       " 536569     üò¢  CRYING FACE  So wake me up when its all over when Im wiser ...\n",
       " 536570     üò¢  CRYING FACE            I wish i could read chinese characters \n",
       " 536571     üò¢  CRYING FACE   Lol eish even on twitter they holding us ransom \n",
       " 536572     üò¢  CRYING FACE             I freakin love you  Ô∏è you are the best\n",
       " \n",
       " [11265 rows x 3 columns],\n",
       " 'EYES':        Emoji Unicode name                                               text\n",
       " 609679     üëÄ         EYES                             Word  i need me one  üèΩ\n",
       " 609680     üëÄ         EYES  forever thankful that shared this wonderful vi...\n",
       " 609681     üëÄ         EYES  Feast your eyes on the trailer for Kaleb amp B...\n",
       " 609682     üëÄ         EYES                        baseball boys do it better \n",
       " 609683     üëÄ         EYES                    Still an entire half to go for \n",
       " ...      ...          ...                                                ...\n",
       " 630498     üëÄ         EYES  The second half of that flew by like  What A Race\n",
       " 630499     üëÄ         EYES  Did my  deceive me or did the Ref just not cal...\n",
       " 630500     üëÄ         EYES  TWO  Performance and interview  Or a game I re...\n",
       " 630501     üëÄ         EYES  A night after his first homer Daniel Vogelbach...\n",
       " 630502     üëÄ         EYES           RT IF YOUR GOING ON THE PITCH NEXT WEEK \n",
       " \n",
       " [20824 rows x 3 columns],\n",
       " 'FACE WITH TEARS OF JOY':        Emoji            Unicode name  \\\n",
       " 0          üòÇ  FACE WITH TEARS OF JOY   \n",
       " 1          üòÇ  FACE WITH TEARS OF JOY   \n",
       " 2          üòÇ  FACE WITH TEARS OF JOY   \n",
       " 3          üòÇ  FACE WITH TEARS OF JOY   \n",
       " 4          üòÇ  FACE WITH TEARS OF JOY   \n",
       " ...      ...                     ...   \n",
       " 210304     üòÇ  FACE WITH TEARS OF JOY   \n",
       " 210305     üòÇ  FACE WITH TEARS OF JOY   \n",
       " 210306     üòÇ  FACE WITH TEARS OF JOY   \n",
       " 210307     üòÇ  FACE WITH TEARS OF JOY   \n",
       " 210308     üòÇ  FACE WITH TEARS OF JOY   \n",
       " \n",
       "                                                      text  \n",
       " 0       Idk who taught my baby this BS  Ô∏è  IGmeetthesa...  \n",
       " 1                               Thats me in every lesson   \n",
       " 2       Partner strategy LLRC  Urban naxal theories ar...  \n",
       " 3                              Dont play with me  üèæ ‚Äç  Ô∏è   \n",
       " 4       To the goofiest boy ever who apparently looks ...  \n",
       " ...                                                   ...  \n",
       " 210304  maybe its one of the tracks but not a single a...  \n",
       " 210305       I honestly cant wait for Big Mouth season 2   \n",
       " 210306  I want 2 bake potatoes nd 2salads with nuggets...  \n",
       " 210307                  true friends wenah wamphe le 50c   \n",
       " 210308     Like a bush of roses take out the fakes Queen   \n",
       " \n",
       " [210309 rows x 3 columns],\n",
       " 'FEMALE SIGN':        Emoji Unicode name                                               text\n",
       " 690836     ‚ôÄ  FEMALE SIGN  Hows your mom a teacher and you cant even spel...\n",
       " 690837     ‚ôÄ  FEMALE SIGN  I can really really really hurt feelings if I ...\n",
       " 690838     ‚ôÄ  FEMALE SIGN   cause hell they only a hand full and them tit...\n",
       " 690839     ‚ôÄ  FEMALE SIGN             UCFs Wakandan Princess periodt  üèΩ ‚Äç  Ô∏è\n",
       " 690840     ‚ôÄ  FEMALE SIGN                    I love my friends üèª ‚Äç  Ô∏è üèª ‚Äç  Ô∏è\n",
       " ...      ...          ...                                                ...\n",
       " 741772     ‚ôÄ  FEMALE SIGN  yall just wont let my Drake amp Chris Brown be...\n",
       " 741773     ‚ôÄ  FEMALE SIGN  Dont let them shame you sis because you have a...\n",
       " 741774     ‚ôÄ  FEMALE SIGN  I only fuck with bitches who got shit to lose ...\n",
       " 741775     ‚ôÄ  FEMALE SIGN  Ive been ghosting a lot of people lately Im so...\n",
       " 741776     ‚ôÄ  FEMALE SIGN      Jackson broke my glasses so Im fucked  üèΩ ‚Äç  Ô∏è\n",
       " \n",
       " [50941 rows x 3 columns],\n",
       " 'FIRE':        Emoji Unicode name                                               text\n",
       " 536573     üî•         FIRE  FIREEE  hugely versatile tracks off YNS Talley...\n",
       " 536574     üî•         FIRE  My girl discovery has LITERALLY changed my lif...\n",
       " 536575     üî•         FIRE  KPOP EXPLOSION Hourly Vote Rate UPDATED STATS ...\n",
       " 536576     üî•         FIRE          VIDEO  performing live on ‚Äò s üáø üá¶ X üá∫ üá∏ ‚Ä¶\n",
       " 536577     üî•         FIRE  REMINDER Episode 1amp2 of BTS  will be release...\n",
       " ...      ...          ...                                                ...\n",
       " 587598     üî•         FIRE   Sixers have won 13 straight for the 1st time ...\n",
       " 587599     üî•         FIRE  Would ask for like 1000 Rts or some but ion ge...\n",
       " 587600     üî•         FIRE  Bring ‚Äò em out Bring ‚Äò em out  RT if youre rea...\n",
       " 587601     üî•         FIRE   NOTIFICATIONS ON for FRESH GAINS  1 ‚É£ LIKE an...\n",
       " 587602     üî•         FIRE               We back at it in the 7 0 4 For FREE \n",
       " \n",
       " [51030 rows x 3 columns],\n",
       " 'FLEXED BICEPS':        Emoji   Unicode name                                               text\n",
       " 398100     üí™  FLEXED BICEPS  my stepbrother wore my other stepbrothers jack...\n",
       " 398101     üí™  FLEXED BICEPS   two infected by chicken pox GWS both you my p...\n",
       " 398102     üí™  FLEXED BICEPS  I promise your fans that I will get knocked 1 ...\n",
       " 398103     üí™  FLEXED BICEPS  Lol twitter is ARMY playground you know  im go...\n",
       " 398104     üí™  FLEXED BICEPS  I finally figured it how to work my ninja blen...\n",
       " ...      ...            ...                                                ...\n",
       " 414729     üí™  FLEXED BICEPS  2018021 2 BKK ICN Have a good sleep today See ...\n",
       " 414730     üí™  FLEXED BICEPS  On to the next round Nice to celebrate another...\n",
       " 414731     üí™  FLEXED BICEPS  K9 Nitro is getting ready to chase bad guys wi...\n",
       " 414732     üí™  FLEXED BICEPS  oof yea I understand completely punch that anx...\n",
       " 414733     üí™  FLEXED BICEPS  This is what we really are  üèª  ‚Äç  Ô∏è  üèª ‚Äç  Ô∏è ‚Äç ...\n",
       " \n",
       " [16634 rows x 3 columns],\n",
       " 'FLUSHED FACE':        Emoji  Unicode name                                               text\n",
       " 501537     üò≥  FLUSHED FACE   I love TheChepo Trendy SALE ENTIRE STORE IS 0...\n",
       " 501538     üò≥  FLUSHED FACE                   shes terrifying omg sign her up \n",
       " 501539     üò≥  FLUSHED FACE  Sunderland are now up for sale for around a qu...\n",
       " 501540     üò≥  FLUSHED FACE              Your father keeps sending me nudes  üèΩ\n",
       " 501541     üò≥  FLUSHED FACE             No clue how Mac McClung finished that \n",
       " ...      ...           ...                                                ...\n",
       " 509981     üò≥  FLUSHED FACE   however they have nothing gluten free Its in ...\n",
       " 509982     üò≥  FLUSHED FACE  God really does show you how ppl are and it ma...\n",
       " 509983     üò≥  FLUSHED FACE  Are You Kidding Benjamin Sanchis is that spec ...\n",
       " 509984     üò≥  FLUSHED FACE  ItalyTheyre not sending their best folks  WTF ...\n",
       " 509985     üò≥  FLUSHED FACE  The track list to Rich The Kids debut album is...\n",
       " \n",
       " [8449 rows x 3 columns],\n",
       " 'GLOWING STAR':        Emoji  Unicode name                                               text\n",
       " 636414     üåü  GLOWING STAR  180408 PUMA HD Take so long time to edit but I...\n",
       " 636415     üåü  GLOWING STAR                     Please RT to vote for our boys\n",
       " 636416     üåü  GLOWING STAR  180128 osaka eclipse concert „Öá„ÖÖ„Öá Is there anyo...\n",
       " 636417     üåü  GLOWING STAR   MY FREE XWEPORN  üÖ≤ üÖª üÖ∏ üÖ≤ üÖ∫ üÖΩ üÖª üÖ∏ üÖΩ üÖ∫ üÜÇ ‚û§ ‚û§ ‚û§ ...\n",
       " 636418     üåü  GLOWING STAR         Follow everyone who retweet and like this \n",
       " ...      ...           ...                                                ...\n",
       " 642598     üåü  GLOWING STAR   HANG TIGHT PATRIOTS REMEMBER Theres power in ...\n",
       " 642599     üåü  GLOWING STAR  Pssst Ô∏è Ô∏è  Want to find out how much celebriti...\n",
       " 642600     üåü  GLOWING STAR   I AM OFFERING COMMISSIONS DM me for more info...\n",
       " 642601     üåü  GLOWING STAR   VOTE  Another day to Vote Dont waste you chan...\n",
       " 642602     üåü  GLOWING STAR   Its official we did it has not only made it i...\n",
       " \n",
       " [6189 rows x 3 columns],\n",
       " 'HEAVY CHECK MARK':        Emoji      Unicode name  \\\n",
       " 630503     ‚úî  HEAVY CHECK MARK   \n",
       " 630504     ‚úî  HEAVY CHECK MARK   \n",
       " 630505     ‚úî  HEAVY CHECK MARK   \n",
       " 630506     ‚úî  HEAVY CHECK MARK   \n",
       " 630507     ‚úî  HEAVY CHECK MARK   \n",
       " ...      ...               ...   \n",
       " 636409     ‚úî  HEAVY CHECK MARK   \n",
       " 636410     ‚úî  HEAVY CHECK MARK   \n",
       " 636411     ‚úî  HEAVY CHECK MARK   \n",
       " 636412     ‚úî  HEAVY CHECK MARK   \n",
       " 636413     ‚úî  HEAVY CHECK MARK   \n",
       " \n",
       "                                                      text  \n",
       " 630503   Mini Giveaway  SATU WINNER GET EXO natrep den...  \n",
       " 630504  Q If the items below fall into water which one...  \n",
       " 630505  CALLING ALL CoD WWII PLAYERS  is recruiting fo...  \n",
       " 630506   Ô∏è The Azerbaijans Best Awarding Ceremony On T...  \n",
       " 630507  Interest within reaps in absolute Bliss  Ô∏è Gre...  \n",
       " ...                                                   ...  \n",
       " 636409  I agree As they want 2 disarm us for a reasons...  \n",
       " 636410  Gain 150 followers tonight extra FAST  Retweet...  \n",
       " 636411  iKONICs  Ô∏è Which team are you on Ô∏è  Ô∏è Team Mel...  \n",
       " 636412  Fresh squeezed flatforms  Ô∏è Spring forward in the  \n",
       " 636413   Check out ALL my gifs of the stunning Kylie Page  \n",
       " \n",
       " [5911 rows x 3 columns],\n",
       " 'LOUDLY CRYING FACE':        Emoji        Unicode name  \\\n",
       " 210309     üò≠  LOUDLY CRYING FACE   \n",
       " 210310     üò≠  LOUDLY CRYING FACE   \n",
       " 210311     üò≠  LOUDLY CRYING FACE   \n",
       " 210312     üò≠  LOUDLY CRYING FACE   \n",
       " 210313     üò≠  LOUDLY CRYING FACE   \n",
       " ...      ...                 ...   \n",
       " 292963     üò≠  LOUDLY CRYING FACE   \n",
       " 292964     üò≠  LOUDLY CRYING FACE   \n",
       " 292965     üò≠  LOUDLY CRYING FACE   \n",
       " 292966     üò≠  LOUDLY CRYING FACE   \n",
       " 292967     üò≠  LOUDLY CRYING FACE   \n",
       " \n",
       "                                                      text  \n",
       " 210309            Why did they do Lil Uzi Vert like that   \n",
       " 210310                           I miss my fitbit so bad   \n",
       " 210311                 two of my fav colleges  shit crazy  \n",
       " 210312  whoo Celebrating 5th ArchiMonthsarywhen your m...  \n",
       " 210313           The dogs reaction when the cat jabs him   \n",
       " ...                                                   ...  \n",
       " 292963                           HUM JEET GAYE FINALLYYY   \n",
       " 292964                      ‚Äú You have one minute left ‚Äù   \n",
       " 292965                               MY HEART IS MELTING   \n",
       " 292966             My cheeks are out and the car is cold   \n",
       " 292967  someone heeelpp idk what to get this kid for h...  \n",
       " \n",
       " [82659 rows x 3 columns],\n",
       " 'MALE SIGN':        Emoji Unicode name                                               text\n",
       " 656687     ‚ôÇ    MALE SIGN                        Its precisely 2 words  ‚Äç  Ô∏è\n",
       " 656688     ‚ôÇ    MALE SIGN  That mf in my group and we got a presentation ...\n",
       " 656689     ‚ôÇ    MALE SIGN  the duck walked üèΩ ‚Äç  Ô∏è up Ô∏è to üèª the lemonade ...\n",
       " 656690     ‚ôÇ    MALE SIGN  Vote T amp G for Prez and VP on April 13 th  üèΩ...\n",
       " 656691     ‚ôÇ    MALE SIGN  When you just got off work tired af bouta pass...\n",
       " ...      ...          ...                                                ...\n",
       " 690831     ‚ôÇ    MALE SIGN  If your reading this I hope something good hap...\n",
       " 690832     ‚ôÇ    MALE SIGN        I just wanna be happy and successful üèæ ‚Äç  Ô∏è\n",
       " 690833     ‚ôÇ    MALE SIGN  Looking for some HBCU kings and queens to help...\n",
       " 690834     ‚ôÇ    MALE SIGN  Anyone up to dm me that cool I have nothing el...\n",
       " 690835     ‚ôÇ    MALE SIGN       Screw me and my overthinking  üèª ‚Äç  Ô∏è  üèª ‚Äç  Ô∏è\n",
       " \n",
       " [34149 rows x 3 columns],\n",
       " 'PARTY POPPER':        Emoji  Unicode name                                               text\n",
       " 429348     üéâ  PARTY POPPER  has reached 20M plays on Anghami are the most ...\n",
       " 429349     üéâ  PARTY POPPER  My week on Twitter  2 Mentions 2 Replies See y...\n",
       " 429350     üéâ  PARTY POPPER                             Happy 25th Birthday  üèΩ\n",
       " 429351     üéâ  PARTY POPPER            Happy Birthday to DO  Catch up with him\n",
       " 429352     üéâ  PARTY POPPER  Happy birthday to my buddy during cavraa days ...\n",
       " ...      ...           ...                                                ...\n",
       " 445602     üéâ  PARTY POPPER  Congratulations Aeris  We won on GDA thank you...\n",
       " 445603     üéâ  PARTY POPPER  CHART 180422 LOCO x HWASA Ï£ºÏßÄÎßà 12AM KST Ranks M...\n",
       " 445604     üéâ  PARTY POPPER  My week on Twitter  10 Mentions 308K Mention R...\n",
       " 445605     üéâ  PARTY POPPER  Only move for feb 10th Ô∏è get yall tickets now ...\n",
       " 445606     üéâ  PARTY POPPER  Texas State students This Thursday the Student...\n",
       " \n",
       " [16259 rows x 3 columns],\n",
       " 'PURPLE HEART':        Emoji  Unicode name                                               text\n",
       " 445607     üíú  PURPLE HEART  Its 1909 ARMY Time Share pics of ARMY  with Be...\n",
       " 445608     üíú  PURPLE HEART                    I love you Always do your best \n",
       " 445609     üíú  PURPLE HEART  Aww taking it with a ppi could help Or low str...\n",
       " 445610     üíú  PURPLE HEART                            Its my mommas birthday \n",
       " 445611     üíú  PURPLE HEART  Seven months with my handsome man today  the h...\n",
       " ...      ...           ...                                                ...\n",
       " 462059     üíú  PURPLE HEART  Brents timeline is even more super today so ma...\n",
       " 462060     üíú  PURPLE HEART  Ah yeah its crazy how missing one day of meds ...\n",
       " 462061     üíú  PURPLE HEART       Love you both so much  have a great day Peep\n",
       " 462062     üíú  PURPLE HEART                              why are we like this \n",
       " 462063     üíú  PURPLE HEART  Just dropped Half Heart  happy vday luv u all ...\n",
       " \n",
       " [16457 rows x 3 columns],\n",
       " 'SKULL':        Emoji Unicode name                                               text\n",
       " 597703     üíÄ        SKULL  its a joke why are you so bothered lmao its no...\n",
       " 597704     üíÄ        SKULL  It done rained snowed and hit 70 ¬∞ and we not ...\n",
       " 597705     üíÄ        SKULL                      Lol Rough tweeting  Let me go\n",
       " 597706     üíÄ        SKULL  Hooked my nigga up with some loud amp then he ...\n",
       " 597707     üíÄ        SKULL              didnt blade make yall the chum bucket\n",
       " ...      ...          ...                                                ...\n",
       " 609674     üíÄ        SKULL  These hoes dont even mind being broke as long ...\n",
       " 609675     üíÄ        SKULL                      Freshman year Vs Senior year \n",
       " 609676     üíÄ        SKULL                  Women hate a dude that is honest \n",
       " 609677     üíÄ        SKULL  Day 3 going gluten free did a 5 mile cardio ch...\n",
       " 609678     üíÄ        SKULL                   Lil fucking liar is all you are \n",
       " \n",
       " [11976 rows x 3 columns],\n",
       " 'SMILING FACE WITH SMILING EYES':        Emoji                    Unicode name  \\\n",
       " 292968     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 292969     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 292970     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 292971     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 292972     üòä  SMILING FACE WITH SMILING EYES   \n",
       " ...      ...                             ...   \n",
       " 321482     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 321483     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 321484     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 321485     üòä  SMILING FACE WITH SMILING EYES   \n",
       " 321486     üòä  SMILING FACE WITH SMILING EYES   \n",
       " \n",
       "                                                      text  \n",
       " 292968           Can I Just Follow Everyone Who Retweets   \n",
       " 292969     Director says that from is his favourite song   \n",
       " 292970  AWAKE Ô∏è Ô∏è Ô∏è Ô∏è  Ô∏è Ô∏è THIS IS OUR WORLDWIDE HANDS...  \n",
       " 292971                   This was the best album he made   \n",
       " 292972               RT if these Two made your childhood   \n",
       " ...                                                   ...  \n",
       " 321482                                    Hi Kris Me too   \n",
       " 321483  One month until the show UK fans please rememb...  \n",
       " 321484                          nun but blessings lately   \n",
       " 321485  The big three releasing songs at the same day ...  \n",
       " 321486   Ô∏è  Ô∏è I didnt see this one coming I love you s...  \n",
       " \n",
       " [28519 rows x 3 columns],\n",
       " 'SMILING FACE WITH SUNGLASSES':        Emoji                  Unicode name  \\\n",
       " 509986     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 509987     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 509988     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 509989     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 509990     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " ...      ...                           ...   \n",
       " 525303     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 525304     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 525305     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 525306     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " 525307     üòé  SMILING FACE WITH SUNGLASSES   \n",
       " \n",
       "                                                      text  \n",
       " 509986  Adrian Kempe welcomes the Golden Knights to ST...  \n",
       " 509987           What I say every morning when I wake up   \n",
       " 509988                 One of the Cleanest Rigs Ive seen   \n",
       " 509989                 Now thiz iz call CLASS  Maal look   \n",
       " 509990                      Aye watch these moves though   \n",
       " ...                                                   ...  \n",
       " 525303  158M  keep up the work guys Shilpa Shinde For ...  \n",
       " 525304                      Throwin shade like its sunny   \n",
       " 525305  If yo boyfriend cheats on you dump his ass dat...  \n",
       " 525306                       DAAAZZZ RIIIGHT  Ô∏è baaabyyy   \n",
       " 525307                           You can read in my mind   \n",
       " \n",
       " [15322 rows x 3 columns],\n",
       " 'SPARKLES':        Emoji Unicode name                                               text\n",
       " 478005     ‚ú®     SPARKLES  share your latest picgif of Jungkook on your p...\n",
       " 478006     ‚ú®     SPARKLES  Whats first impression of each other What are ...\n",
       " 478007     ‚ú®     SPARKLES   Folllow Everyone Who ‚óè ‚ñ¨ ‡πë ‚áì ‡πë ‚ñ¨ ‚ñ¨ ‚ñ¨ ‡πë ‚áì ‡πë ‚ñ¨ ...\n",
       " 478008     ‚ú®     SPARKLES  They just glow together  Ô∏è Our is officially h...\n",
       " 478009     ‚ú®     SPARKLES  The nominees for 2018 British Video are  Sign ...\n",
       " ...      ...          ...                                                ...\n",
       " 501532     ‚ú®     SPARKLES                             I hope has a good day \n",
       " 501533     ‚ú®     SPARKLES  Can your Mom teach my mother and thing or two ...\n",
       " 501534     ‚ú®     SPARKLES   PREMADE COVERS  This one makes me wish I wrot...\n",
       " 501535     ‚ú®     SPARKLES  You are perfect and beautiful armies keep shin...\n",
       " 501536     ‚ú®     SPARKLES  I just notice this uwu egirlss reina and yg ja...\n",
       " \n",
       " [23532 rows x 3 columns],\n",
       " 'SPARKLING HEART':        Emoji     Unicode name  \\\n",
       " 414734     üíñ  SPARKLING HEART   \n",
       " 414735     üíñ  SPARKLING HEART   \n",
       " 414736     üíñ  SPARKLING HEART   \n",
       " 414737     üíñ  SPARKLING HEART   \n",
       " 414738     üíñ  SPARKLING HEART   \n",
       " ...      ...              ...   \n",
       " 429343     üíñ  SPARKLING HEART   \n",
       " 429344     üíñ  SPARKLING HEART   \n",
       " 429345     üíñ  SPARKLING HEART   \n",
       " 429346     üíñ  SPARKLING HEART   \n",
       " 429347     üíñ  SPARKLING HEART   \n",
       " \n",
       "                                                      text  \n",
       " 414734  Witnessing how you keep evolving as an artist ...  \n",
       " 414735             Happy birthday I love you  Be safe Lol  \n",
       " 414736        They have no choice when you are my choice   \n",
       " 414737                               Respect and love on   \n",
       " 414738  Enjoy Our Sale Today Will expire in 24 Hours D...  \n",
       " ...                                                   ...  \n",
       " 429343                           meet the parents  ft amp  \n",
       " 429344                         ICON Happy valentines day   \n",
       " 429345                           No boyfriend No problem   \n",
       " 429346                                  king we love you   \n",
       " 429347                Be my parekoy in a full of bebe ko   \n",
       " \n",
       " [14614 rows x 3 columns],\n",
       " 'TROPHY':        Emoji Unicode name                                               text\n",
       " 642603     üèÜ       TROPHY          MUSIC by yours truly  RT TO THE RAP W RLD\n",
       " 642604     üèÜ       TROPHY  Congratulations to  for winning ‚Üí Best Collabo...\n",
       " 642605     üèÜ       TROPHY   MTV Miaw KPOP EXPLOSION  Estimated Total Twee...\n",
       " 642606     üèÜ       TROPHY  1 Retweet this  2 Follow all that Like amp Ret...\n",
       " 642607     üèÜ       TROPHY  Premier League Champions  So happy where this ...\n",
       " ...      ...          ...                                                ...\n",
       " 649490     üèÜ       TROPHY                A match made in footballing heaven \n",
       " 649491     üèÜ       TROPHY           Who deserves to win a blimp KEEP VOTING \n",
       " 649492     üèÜ       TROPHY  Bob Bryan and Mike Bryan clinch their 116th te...\n",
       " 649493     üèÜ       TROPHY   1 ‚É£ LIKE it Ô∏è 2 ‚É£ FOLLOW all that LIKE amp RE...\n",
       " 649494     üèÜ       TROPHY       BTS Golden Disk Awards Revolution 2014201 6 \n",
       " \n",
       " [6892 rows x 3 columns],\n",
       " 'TWO HEARTS':        Emoji Unicode name                                               text\n",
       " 321487     üíï   TWO HEARTS  hi lovely can you pretty please checkout my Sh...\n",
       " 321488     üíï   TWO HEARTS  Ill post my full sex tape at 500 retweets co o...\n",
       " 321489     üíï   TWO HEARTS           Happy 31st Birthday to Jazmine Sullivan \n",
       " 321490     üíï   TWO HEARTS                                    I love you too \n",
       " 321491     üíï   TWO HEARTS                        You love me and I love you \n",
       " ...      ...          ...                                                ...\n",
       " 351531     üíï   TWO HEARTS                  Happy birthday my boo I love you \n",
       " 351532     üíï   TWO HEARTS     Omg babe you just made my day  I love you too \n",
       " 351533     üíï   TWO HEARTS  Id love to be there I know it will be an aweso...\n",
       " 351534     üíï   TWO HEARTS  Im at a point in my life where cant shit fuck ...\n",
       " 351535     üíï   TWO HEARTS                              They cute or whateva \n",
       " \n",
       " [30049 rows x 3 columns],\n",
       " 'WEARY FACE':        Emoji Unicode name                                               text\n",
       " 351536     üò©   WEARY FACE               Bangers for daysss that turn though \n",
       " 351537     üò©   WEARY FACE  somebody said this the video that caught Lucci...\n",
       " 351538     üò©   WEARY FACE  opens yumecast just to play persona mystery th...\n",
       " 351539     üò©   WEARY FACE  may honors na may high honors pa what an overa...\n",
       " 351540     üò©   WEARY FACE                   And the baby fever has returned \n",
       " ...      ...          ...                                                ...\n",
       " 382020     üò©   WEARY FACE  My mind is always racing with new ideas Its ha...\n",
       " 382021     üò©   WEARY FACE  The games have starved me of some strong Sonic...\n",
       " 382022     üò©   WEARY FACE  Im not from Puerto Rico but BTS IN PUERTO RICO...\n",
       " 382023     üò©   WEARY FACE                 the fact I fw this thread so much \n",
       " 382024     üò©   WEARY FACE  My family stay talking about It but i can neve...\n",
       " \n",
       " [30489 rows x 3 columns],\n",
       " 'WHITE HEAVY CHECK MARK':        Emoji            Unicode name  \\\n",
       " 649495     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 649496     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 649497     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 649498     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 649499     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " ...      ...                     ...   \n",
       " 656682     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 656683     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 656684     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 656685     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " 656686     ‚úÖ  WHITE HEAVY CHECK MARK   \n",
       " \n",
       "                                                      text  \n",
       " 649495  1 Retweet this  2 Follow all that Like amp Ret...  \n",
       " 649496  ‚òÖ STREAM GIVEAWAY ‚òÖ TO WIN  Follow  Retweet Li...  \n",
       " 649497  Twitter Poll 50 Online poll 50  Voting closes ...  \n",
       " 649498  The party continues today  LIKE  COMMENT  RETW...  \n",
       " 649499   The Best list of webcam Sex Cam Sites  100 FR...  \n",
       " ...                                                   ...  \n",
       " 656682   M4A1S I Decimator GIVEAWAY  Follow me and  Ta...  \n",
       " 656683   Three year letterman  68 singles wins 60 doub...  \n",
       " 656684                            Live fast or die young   \n",
       " 656685   Jobs  Increased local expenditure  Tourism Th...  \n",
       " 656686                             ‚Äú WAKANDA Forever üèæ  Ô∏è  \n",
       " \n",
       " [7192 rows x 3 columns],\n",
       " 'WINKING FACE':        Emoji  Unicode name                                               text\n",
       " 382025     üòâ  WINKING FACE  Id definitely pay you a visit whenever I decid...\n",
       " 382026     üòâ  WINKING FACE                    ITS YO BIRTHDAYYY BIIITTCCCHHH \n",
       " 382027     üòâ  WINKING FACE          Nice one Jarred well done ya Commie Cunt \n",
       " 382028     üòâ  WINKING FACE     BANGLADESH ARMYs still there Keep on tweeting \n",
       " 382029     üòâ  WINKING FACE                     Had a busy day signing things \n",
       " ...      ...           ...                                                ...\n",
       " 398095     üòâ  WINKING FACE  If your dream seems unreachable dont lower you...\n",
       " 398096     üòâ  WINKING FACE  big blue finally doing something good with the...\n",
       " 398097     üòâ  WINKING FACE  Hey dont be a dick and use my code on Uber  st...\n",
       " 398098     üòâ  WINKING FACE                        Sounds like a beauty award \n",
       " 398099     üòâ  WINKING FACE  I wonder if any Spurs fans are singing 20 and ...\n",
       " \n",
       " [16075 rows x 3 columns],\n",
       " 'YELLOW HEART':        Emoji  Unicode name                                               text\n",
       " 587603     üíõ  YELLOW HEART  sehuns bday ad at times square in nyc thank yo...\n",
       " 587604     üíõ  YELLOW HEART                     I hope your smile never fades \n",
       " 587605     üíõ  YELLOW HEART     His Majesty always live in our mind eternally \n",
       " 587606     üíõ  YELLOW HEART   Ô∏è  Road to 1B  Ô∏è Lets make 1B hearts on 2018 ...\n",
       " 587607     üíõ  YELLOW HEART  Basically dont listen to your brain if its not...\n",
       " ...      ...           ...                                                ...\n",
       " 597698     üíõ  YELLOW HEART                                Had a great Easter \n",
       " 597699     üíõ  YELLOW HEART       Im such a sweetheart I have a heart of gold \n",
       " 597700     üíõ  YELLOW HEART  Baking homemade granola snack bars and singing...\n",
       " 597701     üíõ  YELLOW HEART  Basta ALL may ALLam ALL na yan  ALLive ALLert ...\n",
       " 597702     üíõ  YELLOW HEART  IF U LIVE IN EUROPEAUSTRALIASIA FINDING ME IS ...\n",
       " \n",
       " [10100 rows x 3 columns]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dictionary made up of dataframes\n",
    "emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630503     Mini Giveaway  SATU WINNER GET EXO natrep den...\n",
       "630504    Q If the items below fall into water which one...\n",
       "630505    CALLING ALL CoD WWII PLAYERS  is recruiting fo...\n",
       "630506     Ô∏è The Azerbaijans Best Awarding Ceremony On T...\n",
       "630507    Interest within reaps in absolute Bliss  Ô∏è Gre...\n",
       "                                ...                        \n",
       "636409    I agree As they want 2 disarm us for a reasons...\n",
       "636410    Gain 150 followers tonight extra FAST  Retweet...\n",
       "636411    iKONICs  Ô∏è Which team are you on Ô∏è  Ô∏è Team Mel...\n",
       "636412    Fresh squeezed flatforms  Ô∏è Spring forward in the\n",
       "636413     Check out ALL my gifs of the stunning Kylie Page\n",
       "Name: text, Length: 5911, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying\n",
    "emoji_dict['HEAVY CHECK MARK']['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Send tweets as input to the pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify emoji emotions and save in new emotions column\n",
    "for key in emoji_dict:\n",
    "    emoji_dict[key]['Emotions'] = emoji_dict[key]['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "    emoji_dict[key].drop(['text', 'Unicode name'], axis=1, inplace=True)\n",
    "\n",
    "# This takes a while, so instead of having my machine occupied for the number of hours this needs to finish, i broke it down into smaller pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify emoji emotions and save in new emotions column\n",
    "emoji_dict['HEAVY CHECK MARK']['Emotions'] = emoji_dict['HEAVY CHECK MARK']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "# drop tweet column\n",
    "emoji_dict['HEAVY CHECK MARK'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOWING STAR \n",
    "emoji_dict['GLOWING STAR']['Emotions'] = emoji_dict['GLOWING STAR']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['GLOWING STAR'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TROPHY\n",
    "emoji_dict['TROPHY']['Emotions'] = emoji_dict['TROPHY']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['TROPHY'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHITE HEAVY CHECK MARK\n",
    "emoji_dict['WHITE HEAVY CHECK MARK']['Emotions'] = emoji_dict['WHITE HEAVY CHECK MARK']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['WHITE HEAVY CHECK MARK'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUSHED FACE\n",
    "emoji_dict['FLUSHED FACE']['Emotions'] = emoji_dict['FLUSHED FACE']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['FLUSHED FACE'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YELLOW HEART\n",
    "emoji_dict['YELLOW HEART']['Emotions'] = emoji_dict['YELLOW HEART']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['YELLOW HEART'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRYING FACE\n",
    "emoji_dict['CRYING FACE']['Emotions'] = emoji_dict['CRYING FACE']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['CRYING FACE'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKULL\n",
    "emoji_dict['SKULL']['Emotions'] = emoji_dict['SKULL']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['SKULL'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARKLING HEART\n",
    "emoji_dict['SPARKLING HEART']['Emotions'] = emoji_dict['SPARKLING HEART']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['SPARKLING HEART'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILING FACE WITH SUNGLASSES\n",
    "emoji_dict['SMILING FACE WITH SUNGLASSES']['Emotions'] = emoji_dict['SMILING FACE WITH SUNGLASSES']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['SMILING FACE WITH SUNGLASSES'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLUE HEART\n",
    "emoji_dict['BLUE HEART']['Emotions'] = emoji_dict['BLUE HEART']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['BLUE HEART'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINKING FACE\n",
    "emoji_dict['WINKING FACE']['Emotions'] = emoji_dict['WINKING FACE']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['WINKING FACE'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTY POPPER\n",
    "emoji_dict['PARTY POPPER']['Emotions'] = emoji_dict['PARTY POPPER']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['PARTY POPPER'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPLE HEART\n",
    "emoji_dict['PURPLE HEART']['Emotions'] = emoji_dict['PURPLE HEART']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['PURPLE HEART'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLEXED BICEPS\n",
    "emoji_dict['FLEXED BICEPS']['Emotions'] = emoji_dict['FLEXED BICEPS']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['FLEXED BICEPS'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EYES\n",
    "emoji_dict['EYES']['Emotions'] = emoji_dict['EYES']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['EYES'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARKLES\n",
    "emoji_dict['SPARKLES']['Emotions'] = emoji_dict['SPARKLES']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['SPARKLES'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILING FACE WITH SMILING EYES\n",
    "emoji_dict['SMILING FACE WITH SMILING EYES']['Emotions'] = emoji_dict['SMILING FACE WITH SMILING EYES']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['SMILING FACE WITH SMILING EYES'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWO HEARTS\n",
    "emoji_dict['TWO HEARTS']['Emotions'] = emoji_dict['TWO HEARTS']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['TWO HEARTS'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEARY FACE\n",
    "emoji_dict['WEARY FACE']['Emotions'] = emoji_dict['WEARY FACE']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['WEARY FACE'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALE SIGN\n",
    "emoji_dict['MALE SIGN']['Emotions'] = emoji_dict['MALE SIGN']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['MALE SIGN'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMALE SIGN \n",
    "emoji_dict['FEMALE SIGN']['Emotions'] = emoji_dict['FEMALE SIGN']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['FEMALE SIGN'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRE\n",
    "emoji_dict['FIRE']['Emotions'] = emoji_dict['FIRE']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['FIRE'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOUDLY CRYING FACE\n",
    "emoji_dict['LOUDLY CRYING FACE']['Emotions'] = emoji_dict['LOUDLY CRYING FACE']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['LOUDLY CRYING FACE'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE WITH TEARS OF JOY\n",
    "emoji_dict['FACE WITH TEARS OF JOY']['Emotions'] = emoji_dict['FACE WITH TEARS OF JOY']['text'].apply(lambda tweet: get_emotion(tweet.lower()))\n",
    "emoji_dict['FACE WITH TEARS OF JOY'].drop(['text', 'Unicode name'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete extra word present at the start of the string\n",
    "for key in emoji_dict:    \n",
    "    emoji_dict[key]['Emotions'] = emoji_dict[key]['Emotions'].apply(lambda tweet: tweet.split(' ', 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMALE SIGN\n",
    "emoji_dict['FEMALE SIGN']['Emotions'] = emoji_dict['FEMALE SIGN']['Emotions'].apply(lambda tweet: tweet.split(' ', 1)[1])\n",
    "# FIRE\n",
    "emoji_dict['FIRE']['Emotions'] = emoji_dict['FIRE']['Emotions'].apply(lambda tweet: tweet.split(' ', 1)[1])\n",
    "# LOUDLY CRYING FACE\n",
    "emoji_dict['LOUDLY CRYING FACE']['Emotions'] = emoji_dict['LOUDLY CRYING FACE']['Emotions'].apply(lambda tweet: tweet.split(' ', 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE WITH TEARS OF JOY\n",
    "emoji_dict['FACE WITH TEARS OF JOY']['Emotions'] = emoji_dict['FACE WITH TEARS OF JOY']['Emotions'].apply(lambda tweet: tweet.split(' ', 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dictionary\n",
    "import csv\n",
    "with open('../../../Desktop/capstone-data/emoji_emotion_dict.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in emoji_dict.items():\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dictionary dfs into one df\n",
    "df_list = []\n",
    "for key in emoji_dict:\n",
    "    df_list.append(emoji_dict[key])\n",
    "\n",
    "df_emoji_emotion = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emoji_emotion.Emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emoji_emotion[['Emotion'] == 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>538954</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üî•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363022</th>\n",
       "      <td>joy</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248599</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552904</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üî•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128696</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439653</th>\n",
       "      <td>joy</td>\n",
       "      <td>üéâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408399</th>\n",
       "      <td>fear</td>\n",
       "      <td>üí™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684824</th>\n",
       "      <td>fear</td>\n",
       "      <td>‚ôÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296115</th>\n",
       "      <td>anger</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473219</th>\n",
       "      <td>love</td>\n",
       "      <td>üíô</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>‚ôÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578844</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üî•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377609</th>\n",
       "      <td>sadness</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626141</th>\n",
       "      <td>anger</td>\n",
       "      <td>üëÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274676</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584254</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üî•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454557</th>\n",
       "      <td>sadness</td>\n",
       "      <td>üíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457677</th>\n",
       "      <td>anger</td>\n",
       "      <td>üíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530303</th>\n",
       "      <td>joy</td>\n",
       "      <td>üò¢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484778</th>\n",
       "      <td>joy</td>\n",
       "      <td>‚ú®</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242535</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428045</th>\n",
       "      <td>anger</td>\n",
       "      <td>üíñ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79239</th>\n",
       "      <td>NaN</td>\n",
       "      <td>üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696425</th>\n",
       "      <td>NaN</td>\n",
       "      <td>‚ôÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372506</th>\n",
       "      <td>sadness</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517220</th>\n",
       "      <td>joy</td>\n",
       "      <td>üòé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500976</th>\n",
       "      <td>love</td>\n",
       "      <td>‚ú®</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389182</th>\n",
       "      <td>joy</td>\n",
       "      <td>üòâ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Emotions Emoji\n",
       "538954      NaN     üî•\n",
       "363022      joy     üò©\n",
       "248599      NaN     üò≠\n",
       "552904      NaN     üî•\n",
       "128696      NaN     üòÇ\n",
       "439653      joy     üéâ\n",
       "408399     fear     üí™\n",
       "684824     fear     ‚ôÇ\n",
       "296115    anger     üòä\n",
       "473219     love     üíô\n",
       "1319        NaN     üòÇ\n",
       "713881      NaN     ‚ôÄ\n",
       "578844      NaN     üî•\n",
       "377609  sadness     üò©\n",
       "626141    anger     üëÄ\n",
       "274676      NaN     üò≠\n",
       "584254      NaN     üî•\n",
       "454557  sadness     üíú\n",
       "457677    anger     üíú\n",
       "530303      joy     üò¢\n",
       "484778      joy     ‚ú®\n",
       "242535      NaN     üò≠\n",
       "428045    anger     üíñ\n",
       "256574      NaN     üò≠\n",
       "79239       NaN     üòÇ\n",
       "696425      NaN     ‚ôÄ\n",
       "372506  sadness     üò©\n",
       "517220      joy     üòé\n",
       "500976     love     ‚ú®\n",
       "389182      joy     üòâ"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show random 30 rows to check data quality\n",
    "df_emoji_emotion[['Emotions', 'Emoji']].sample(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset as csv\n",
    "df_emoji_emotion.to_csv('../../../Desktop/capstone-data/emoji_emotion_df.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Merge emoji and emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = pd.read_csv(\"../data/emotions/train.txt\", delimiter=';', header=None, names=['Sentence','Emotions'])\n",
    "emoji_df = pd.read_csv('../../../Desktop/capstone-data/emoji_emotion_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Emotions\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show first 5 rows\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df.Emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>630503</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630504</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630505</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630506</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630507</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Emoji Emotions\n",
       "630503     ‚úî      joy\n",
       "630504     ‚úî      joy\n",
       "630505     ‚úî      joy\n",
       "630506     ‚úî      joy\n",
       "630507     ‚úî      joy"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two tables on 'Emotion' column\n",
    "emotion_emoji_merged = emoji_df.merge(emotion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i do feel that running is a divine experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have immense sympathy with the general point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i do not feel reassured anxiety is on each side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have the feeling she was amused and delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i was able to help chai lifeline with your sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i feel more superior dead chicken or grieving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i get giddy over feeling elegant in a perfectl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i can t imagine a real life scenario where i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i am not sure what would make me feel content ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have been feeling the need to be creative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i do however want you to know that if somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i need you i need someone i need to be protect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i plan to share my everyday life stories trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i already have my christmas trees up i got two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>ive worn it once on its own with a little conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i feel very strongly passionate about when som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i lost my special mind but don t worry i m sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>on a boat trip to denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i need to feel the dough to make sure its just...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emoji Emotions                                           Sentence\n",
       "0      ‚úî      joy  i have been with petronas for years i feel tha...\n",
       "1      ‚úî      joy  i do feel that running is a divine experience ...\n",
       "2      ‚úî      joy  i have immense sympathy with the general point...\n",
       "3      ‚úî      joy    i do not feel reassured anxiety is on each side\n",
       "4      ‚úî      joy    i have the feeling she was amused and delighted\n",
       "5      ‚úî      joy  i was able to help chai lifeline with your sup...\n",
       "6      ‚úî      joy  i feel more superior dead chicken or grieving ...\n",
       "7      ‚úî      joy  i get giddy over feeling elegant in a perfectl...\n",
       "8      ‚úî      joy  i can t imagine a real life scenario where i w...\n",
       "9      ‚úî      joy  i am not sure what would make me feel content ...\n",
       "10     ‚úî      joy        i have been feeling the need to be creative\n",
       "11     ‚úî      joy  i do however want you to know that if somethin...\n",
       "12     ‚úî      joy  i need you i need someone i need to be protect...\n",
       "13     ‚úî      joy  i plan to share my everyday life stories trave...\n",
       "14     ‚úî      joy  i already have my christmas trees up i got two...\n",
       "15     ‚úî      joy  ive worn it once on its own with a little conc...\n",
       "16     ‚úî      joy  i feel very strongly passionate about when som...\n",
       "17     ‚úî      joy  i lost my special mind but don t worry i m sti...\n",
       "18     ‚úî      joy                          on a boat trip to denmark\n",
       "19     ‚úî      joy  i need to feel the dough to make sure its just..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_emoji_merged.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26327103"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_emoji_merged.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_emoji_merged = emotion_emoji_merged.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15999, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_emoji_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i do feel that running is a divine experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have immense sympathy with the general point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i do not feel reassured anxiety is on each side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have the feeling she was amused and delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i was able to help chai lifeline with your sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i feel more superior dead chicken or grieving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i get giddy over feeling elegant in a perfectl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i can t imagine a real life scenario where i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i am not sure what would make me feel content ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i have been feeling the need to be creative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i do however want you to know that if somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i need you i need someone i need to be protect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i plan to share my everyday life stories trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i already have my christmas trees up i got two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>ive worn it once on its own with a little conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i feel very strongly passionate about when som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i lost my special mind but don t worry i m sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>on a boat trip to denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>‚úî</td>\n",
       "      <td>joy</td>\n",
       "      <td>i need to feel the dough to make sure its just...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emoji Emotions                                           Sentence\n",
       "0      ‚úî      joy  i have been with petronas for years i feel tha...\n",
       "1      ‚úî      joy  i do feel that running is a divine experience ...\n",
       "2      ‚úî      joy  i have immense sympathy with the general point...\n",
       "3      ‚úî      joy    i do not feel reassured anxiety is on each side\n",
       "4      ‚úî      joy    i have the feeling she was amused and delighted\n",
       "5      ‚úî      joy  i was able to help chai lifeline with your sup...\n",
       "6      ‚úî      joy  i feel more superior dead chicken or grieving ...\n",
       "7      ‚úî      joy  i get giddy over feeling elegant in a perfectl...\n",
       "8      ‚úî      joy  i can t imagine a real life scenario where i w...\n",
       "9      ‚úî      joy  i am not sure what would make me feel content ...\n",
       "10     ‚úî      joy        i have been feeling the need to be creative\n",
       "11     ‚úî      joy  i do however want you to know that if somethin...\n",
       "12     ‚úî      joy  i need you i need someone i need to be protect...\n",
       "13     ‚úî      joy  i plan to share my everyday life stories trave...\n",
       "14     ‚úî      joy  i already have my christmas trees up i got two...\n",
       "15     ‚úî      joy  ive worn it once on its own with a little conc...\n",
       "16     ‚úî      joy  i feel very strongly passionate about when som...\n",
       "17     ‚úî      joy  i lost my special mind but don t worry i m sti...\n",
       "18     ‚úî      joy                          on a boat trip to denmark\n",
       "19     ‚úî      joy  i need to feel the dough to make sure its just..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_emoji_merged.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         5361\n",
       "sadness     4666\n",
       "anger       2159\n",
       "fear        1937\n",
       "love        1304\n",
       "surprise     572\n",
       "Name: Emotions, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_emoji_merged['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset as csv\n",
    "emotion_emoji_merged.to_csv('../../../Desktop/capstone-data/emoji_emotion_merged.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Text emoji recommendation\n",
    "In this lat part, we train a new model to take the text and recommends an emoji based on that text. The feature is the text and the target variable is the emoji. First we're going to apply multinomial naive bayes classifier which is one of the most popular machine learning algorithm in natural language processing., then try building a deep learning neural network and compare results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from tkinter import *\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python import keras\n",
    "import string\n",
    "from keras.layers import Dense, Activation, Input, Dropout, SimpleRNN, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "emotion_emoji_merged = pd.read_csv('../../../Desktop/capstone-data/emoji_emotion_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_emoji_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15999,) (15999,)\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "X = emotion_emoji_merged['Sentence']\n",
    "y = emotion_emoji_merged['Emoji']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(emotion_emoji_merged, x=\"Emotions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix using golve vectors from pre-trained models\n",
    "file = open(\"../../../Desktop/glove.6B/glove.6B.50d.txt\", encoding = 'utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates a dictionary where the key is the word and the values are all the embeddings\n",
    "# the dictionary that will hold the mappings between words, and the embedding vectors of those words\n",
    "def intialize_emb_matrix(file):\n",
    "    embedding_matrix = {}\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.array(values[1:], dtype='float64')\n",
    "        embedding_matrix[word] = embedding\n",
    "\n",
    "    return embedding_matrix \n",
    "embedding_matrix = intialize_emb_matrix(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53646 , -0.072432,  0.24182 ,  0.099021,  0.18426 , -0.86764 ,\n",
       "        0.081939,  0.40473 , -0.40506 ,  0.47446 , -0.16865 ,  0.38936 ,\n",
       "       -0.16916 ,  0.1661  ,  0.73543 ,  0.83612 ,  0.026771,  0.56956 ,\n",
       "        0.41988 , -0.23297 , -0.58841 ,  0.5495  ,  0.71645 ,  0.22451 ,\n",
       "        1.0043  , -1.5036  , -0.78521 ,  0.73364 ,  0.4161  , -1.6782  ,\n",
       "        1.9156  ,  0.26593 , -0.41546 ,  0.97965 , -0.06039 , -0.74422 ,\n",
       "        0.6166  , -0.023109,  0.77383 , -0.65267 , -0.20022 , -0.2479  ,\n",
       "        0.04704 ,  0.31407 ,  0.32598 , -0.24481 ,  0.16835 ,  0.097793,\n",
       "        0.12392 ,  1.1584  ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the vector of the word‚Äôs position\n",
    "embedding_matrix['ok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding our text dataset\n",
    "# note: we shouldn't remove stop words before embedding \n",
    "# because that would remove semantic structure \n",
    "# that we need for our model to work well\n",
    "def get_emb_data(data, max_len):\n",
    "    embedding_data = np.zeros((len(data), max_len, 50))  # from glove6B50d\n",
    "    \n",
    "    for idx in range(data.shape[0]):\n",
    "        words_in_sentence = data[idx].split()\n",
    "        \n",
    "        for i in range(len(words_in_sentence)):\n",
    "            if embedding_matrix.get(words_in_sentence[i].lower()) is not None:\n",
    "                embedding_data[idx][i] = embedding_matrix[words_in_sentence[i].lower()]\n",
    "                \n",
    "    return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding for train daata\n",
    "X_train_emb = get_emb_data(X_train, 168)\n",
    "# training data after embedding\n",
    "X_train_emb\n",
    "# get embedding for test data\n",
    "X_test_emb = get_emb_data(X_test, 168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embedding_matrix.keys(), key=lambda word: distance.euclidean(embedding_matrix[word], embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper',\n",
       " 'print',\n",
       " 'sheet',\n",
       " 'printed',\n",
       " 'printing',\n",
       " 'ink',\n",
       " 'papers',\n",
       " 'copy',\n",
       " 'cover',\n",
       " 'contents',\n",
       " 'contained',\n",
       " 'sheets',\n",
       " 'piece',\n",
       " 'notes',\n",
       " 'covered',\n",
       " 'made',\n",
       " 'material',\n",
       " 'packaging',\n",
       " 'laid',\n",
       " 'reference',\n",
       " 'instead',\n",
       " 'stamped',\n",
       " 'instance',\n",
       " 'covering',\n",
       " 'explaining',\n",
       " 'recycled',\n",
       " 'distributed',\n",
       " 'press',\n",
       " 'discarded',\n",
       " 'read',\n",
       " 'publish',\n",
       " 'collected',\n",
       " '.',\n",
       " 'materials',\n",
       " 'box',\n",
       " 'publication',\n",
       " 'placing',\n",
       " 'paint',\n",
       " 'pointed',\n",
       " 'pointing',\n",
       " 'page',\n",
       " 'putting',\n",
       " 'publishing',\n",
       " 'supplied',\n",
       " 'making',\n",
       " 'aside',\n",
       " 'showing',\n",
       " 'selling',\n",
       " 'soft',\n",
       " 'own',\n",
       " 'suggested',\n",
       " 'note',\n",
       " 'using',\n",
       " 'suggesting',\n",
       " 'adding',\n",
       " 'rolls',\n",
       " 'provided',\n",
       " 'post',\n",
       " 'collection',\n",
       " 'put',\n",
       " 'newspapers',\n",
       " 'advertisement',\n",
       " 'bearing',\n",
       " 'product',\n",
       " 'collecting',\n",
       " 'pieces',\n",
       " 'articles',\n",
       " 'mirror',\n",
       " 'merely',\n",
       " 'fake',\n",
       " 'reading',\n",
       " 'hand',\n",
       " 'labeled',\n",
       " 'example',\n",
       " 'simply',\n",
       " 'carried',\n",
       " 'roll',\n",
       " 'picture',\n",
       " 'partly',\n",
       " 'recently',\n",
       " 'used',\n",
       " 'book',\n",
       " 'changed',\n",
       " 'essentially',\n",
       " 'delivered',\n",
       " 'pressed',\n",
       " 'illustrated',\n",
       " 'account',\n",
       " 'filled',\n",
       " 'on',\n",
       " 'well',\n",
       " 'describing',\n",
       " 'drawn',\n",
       " 'kept',\n",
       " 'fine',\n",
       " 'according',\n",
       " 'revealing',\n",
       " 'same',\n",
       " 'article',\n",
       " 'magazine',\n",
       " 'which',\n",
       " 'signature',\n",
       " 'with',\n",
       " 'strips',\n",
       " 'document',\n",
       " 'check',\n",
       " 'pen',\n",
       " 'comparing',\n",
       " 'letters',\n",
       " 'directly',\n",
       " 'obtained',\n",
       " 'scraps',\n",
       " 'tape',\n",
       " 'uses',\n",
       " 'created',\n",
       " 'suggests',\n",
       " 'dropping',\n",
       " 'covers',\n",
       " 'as',\n",
       " 'reveal',\n",
       " 'boxes',\n",
       " 'editorial',\n",
       " 'volumes',\n",
       " 'by',\n",
       " 'labels',\n",
       " 'or',\n",
       " 'today',\n",
       " 'besides',\n",
       " 'raised',\n",
       " 'envelope',\n",
       " 'holding',\n",
       " 'also',\n",
       " 'rolling',\n",
       " 'source',\n",
       " 'showed',\n",
       " 'fashioned',\n",
       " 'items',\n",
       " 'sells',\n",
       " 'supposedly',\n",
       " 'addition',\n",
       " 'keeps',\n",
       " 'published',\n",
       " 'placed',\n",
       " 'it',\n",
       " 'books',\n",
       " 'containing',\n",
       " 'cardboard',\n",
       " 'magazines',\n",
       " 'keeping',\n",
       " 'works',\n",
       " 'removed',\n",
       " 'cutting',\n",
       " 'whole',\n",
       " ',',\n",
       " 'introduction',\n",
       " 'except',\n",
       " 'cut',\n",
       " 'stacks',\n",
       " 'indicating',\n",
       " 'similarly',\n",
       " 'raw',\n",
       " 'carefully',\n",
       " 'posted',\n",
       " 'likewise',\n",
       " 'etc.',\n",
       " 'additionally',\n",
       " 'pamphlet',\n",
       " 'moreover',\n",
       " 'referred',\n",
       " 'item',\n",
       " 'basically',\n",
       " 'turning',\n",
       " 'and',\n",
       " 'bulk',\n",
       " 'presses',\n",
       " 'done',\n",
       " 'presented',\n",
       " 'collects',\n",
       " 'prints',\n",
       " 'blank',\n",
       " 'removing',\n",
       " 'applied',\n",
       " 'included',\n",
       " 'wrapped',\n",
       " 'documents',\n",
       " 'explained',\n",
       " 'times',\n",
       " 'turned',\n",
       " 'displayed',\n",
       " 'referring',\n",
       " 'glass',\n",
       " 'detailing',\n",
       " 'photo',\n",
       " 'full',\n",
       " 'revealed',\n",
       " 'advertising',\n",
       " 'boards',\n",
       " 'publishes',\n",
       " 'plastic',\n",
       " 'reads',\n",
       " 'suggest',\n",
       " 'meant',\n",
       " 'noting',\n",
       " 'framed',\n",
       " 'makes',\n",
       " 'everything',\n",
       " 'stating',\n",
       " 'attached',\n",
       " 'stands',\n",
       " 'accompanying',\n",
       " 'actually',\n",
       " 'brush',\n",
       " '‚Äù',\n",
       " 'mixed',\n",
       " 'found',\n",
       " 'volume',\n",
       " 'coin',\n",
       " 'stamp',\n",
       " 'giving',\n",
       " 'separately',\n",
       " 'comes',\n",
       " 'clear',\n",
       " 'idea',\n",
       " 'accounts',\n",
       " 'drawing',\n",
       " 'laying',\n",
       " 'issue',\n",
       " 'introduced',\n",
       " 'similar',\n",
       " 'circulated',\n",
       " 'dot',\n",
       " 'added',\n",
       " 'purpose',\n",
       " 'rather',\n",
       " 'background',\n",
       " 'borrowed',\n",
       " 'refers',\n",
       " 'relied',\n",
       " 'fact',\n",
       " 'partially',\n",
       " 'wrapping',\n",
       " 'seal',\n",
       " 'canvas',\n",
       " 'for',\n",
       " 'pages',\n",
       " 'available',\n",
       " 'that',\n",
       " 'taken',\n",
       " 'purchasing',\n",
       " 'from',\n",
       " 'gave',\n",
       " 'noted',\n",
       " 'candy',\n",
       " 'extracts',\n",
       " 'advertised',\n",
       " 'while',\n",
       " 'stuck',\n",
       " 'although',\n",
       " 'proof',\n",
       " 'letter',\n",
       " 'use',\n",
       " 'straw',\n",
       " 'stack',\n",
       " 'flat',\n",
       " 'invented',\n",
       " 'widely',\n",
       " 'touch',\n",
       " 'topped',\n",
       " 'comparison',\n",
       " 'poster',\n",
       " 'media',\n",
       " 'called',\n",
       " 'literally',\n",
       " 'saw',\n",
       " 'publications',\n",
       " 'photograph',\n",
       " 'puts',\n",
       " 'artwork',\n",
       " 'presenting',\n",
       " 'picking',\n",
       " 'holds',\n",
       " 'sticking',\n",
       " 'prominently',\n",
       " 'text',\n",
       " '‚Äú',\n",
       " 'bulletin',\n",
       " 'insert',\n",
       " 'scrap',\n",
       " 'listing',\n",
       " 'compiled',\n",
       " 'this',\n",
       " 'sale',\n",
       " 'products',\n",
       " 'stick',\n",
       " 'nonetheless',\n",
       " 'though',\n",
       " 'thin',\n",
       " 'hands',\n",
       " 'filling',\n",
       " 'entire',\n",
       " 'lists',\n",
       " 'writing',\n",
       " 'entirely',\n",
       " 'sources',\n",
       " 'originally',\n",
       " 'indeed',\n",
       " 'displaying',\n",
       " 'remove',\n",
       " 'work',\n",
       " 'posts',\n",
       " 'short',\n",
       " 'vanity',\n",
       " 'incorrectly',\n",
       " 'brought',\n",
       " 'without',\n",
       " 'relying',\n",
       " 'shows',\n",
       " 'anonymously',\n",
       " 'cited',\n",
       " 'furthermore',\n",
       " 'introducing',\n",
       " 'set',\n",
       " 'exclusively',\n",
       " 'its',\n",
       " 'once',\n",
       " 'however',\n",
       " 'embedded',\n",
       " 'green',\n",
       " 'releases',\n",
       " 'indicate',\n",
       " 'plates',\n",
       " 'large',\n",
       " 'intended',\n",
       " 'phony',\n",
       " 'calls',\n",
       " 'the',\n",
       " 'indicates',\n",
       " 'forged',\n",
       " 'core',\n",
       " 'rolled',\n",
       " 'drawings',\n",
       " 'prepared',\n",
       " 'separate',\n",
       " 'setting',\n",
       " 'up',\n",
       " 'addressed',\n",
       " 'coated',\n",
       " 'newspaper',\n",
       " 'logs',\n",
       " 'picked',\n",
       " 'offered',\n",
       " 'wall',\n",
       " 'presumably',\n",
       " 'presents',\n",
       " 'scoop',\n",
       " 'under',\n",
       " 'cracked',\n",
       " 'turn',\n",
       " 'sending',\n",
       " 'suggestion',\n",
       " 'spread',\n",
       " 'appears',\n",
       " 'pencil',\n",
       " 'cloth',\n",
       " 'frequently',\n",
       " 'original',\n",
       " 'write',\n",
       " 'records',\n",
       " 'focusing',\n",
       " 'essence',\n",
       " 'mention',\n",
       " 'indicated',\n",
       " 'machine',\n",
       " 'stationery',\n",
       " 'one',\n",
       " 'followed',\n",
       " 'produced',\n",
       " 'nevertheless',\n",
       " 'standard',\n",
       " 'still',\n",
       " 'shown',\n",
       " 'entitled',\n",
       " 'packaged',\n",
       " 'apart',\n",
       " 'unlike',\n",
       " 'few',\n",
       " 'onto',\n",
       " 'advertisements',\n",
       " 'shaped',\n",
       " 'new',\n",
       " 'stated',\n",
       " 'catalog',\n",
       " 'contrast',\n",
       " 'came',\n",
       " 'register',\n",
       " 'leaked',\n",
       " 'wood',\n",
       " 'bought',\n",
       " 'submitted',\n",
       " 'booklet',\n",
       " 'changing',\n",
       " 'forth',\n",
       " 'wax',\n",
       " 'business',\n",
       " 'sold',\n",
       " 'purchased',\n",
       " 'bag',\n",
       " 'tool',\n",
       " 'translated',\n",
       " 'pulp',\n",
       " 'slogan',\n",
       " 'heavily',\n",
       " 'impressions',\n",
       " 'table',\n",
       " 'detailed',\n",
       " 'moving',\n",
       " 'white',\n",
       " 'diary',\n",
       " 'loose',\n",
       " 'titled',\n",
       " 'counting',\n",
       " 'specifically',\n",
       " 'custom',\n",
       " 'facts',\n",
       " 'figures',\n",
       " 'handled',\n",
       " '[',\n",
       " 'supplement',\n",
       " 'word',\n",
       " 'maintained',\n",
       " 'exception',\n",
       " 'circulating',\n",
       " 'wallpaper',\n",
       " 'information',\n",
       " 'quality',\n",
       " 'engraving',\n",
       " 'makers',\n",
       " 'pile',\n",
       " 'previously',\n",
       " 'pictures',\n",
       " 'bogus',\n",
       " 'devoted',\n",
       " 'reproduced',\n",
       " 'creating',\n",
       " 'copied',\n",
       " 'invention',\n",
       " 'report',\n",
       " 'supposed',\n",
       " 'meanwhile',\n",
       " 'either',\n",
       " 'reports',\n",
       " 'now',\n",
       " 'label',\n",
       " 'seen',\n",
       " 'present',\n",
       " 'calling',\n",
       " 'fabricated',\n",
       " 'plate',\n",
       " 'wrap',\n",
       " 'matching',\n",
       " 'electronic',\n",
       " 'cites',\n",
       " 'particular',\n",
       " 'incorporating',\n",
       " 'immediately',\n",
       " 'plain',\n",
       " 'clean',\n",
       " 'subject',\n",
       " 'handmade',\n",
       " 'signs',\n",
       " 'cookie',\n",
       " 'nor',\n",
       " 'clothing',\n",
       " 'reviewed',\n",
       " 'given',\n",
       " 'notice',\n",
       " 'but',\n",
       " 'delivering',\n",
       " 'posting',\n",
       " 'pens',\n",
       " 'image',\n",
       " 'through',\n",
       " 'name',\n",
       " 'pre',\n",
       " 'firm',\n",
       " 'itself',\n",
       " 'focused',\n",
       " 'looking',\n",
       " 'handing',\n",
       " 'contain',\n",
       " 'barely',\n",
       " 'begun',\n",
       " 'personal',\n",
       " 'solid',\n",
       " 'clipping',\n",
       " 'means',\n",
       " 'described',\n",
       " 'a',\n",
       " 'describes',\n",
       " 'already',\n",
       " 'handles',\n",
       " 'memo',\n",
       " 'meantime',\n",
       " 'examining',\n",
       " 'mixing',\n",
       " 'telegraph',\n",
       " 'casting',\n",
       " 'vendor',\n",
       " 'later',\n",
       " 'solely',\n",
       " 'quickly',\n",
       " 'laminated',\n",
       " 'upon',\n",
       " 'listed',\n",
       " 'telling',\n",
       " 'neither',\n",
       " 'underneath',\n",
       " 'wire',\n",
       " 'to',\n",
       " 'false',\n",
       " 'names',\n",
       " 'form',\n",
       " 'website',\n",
       " 'appeared',\n",
       " 'pressing',\n",
       " 'gives',\n",
       " 'description',\n",
       " 'headline',\n",
       " 'folded',\n",
       " 'carry',\n",
       " 'counter',\n",
       " 'initially',\n",
       " 'secret',\n",
       " 'keep',\n",
       " 'commonly',\n",
       " 'cards',\n",
       " 'faux',\n",
       " 'colored',\n",
       " 'issued',\n",
       " 'banner',\n",
       " 'manufactured',\n",
       " 'regularly',\n",
       " 'closely',\n",
       " 'hidden',\n",
       " 'details',\n",
       " 'even',\n",
       " 'usually',\n",
       " 'altered',\n",
       " 'attention',\n",
       " 'finger',\n",
       " 'every',\n",
       " 'catalogue',\n",
       " 'ups',\n",
       " 'reflected',\n",
       " 'traced',\n",
       " 'releasing',\n",
       " 'branding',\n",
       " 'obscure',\n",
       " 'reprinted',\n",
       " 'pictured',\n",
       " 'saying',\n",
       " 'checking',\n",
       " 'earlier',\n",
       " 'slips',\n",
       " 'handling',\n",
       " 'over',\n",
       " 'follows',\n",
       " 'reporting',\n",
       " 'pulling',\n",
       " 'beyond',\n",
       " 'another',\n",
       " 'ran',\n",
       " 'mix',\n",
       " 'factory',\n",
       " 'unfortunately',\n",
       " 'hanging',\n",
       " 'content',\n",
       " 'supplemented',\n",
       " 'sent',\n",
       " 'commented',\n",
       " 'announcing',\n",
       " 'applying',\n",
       " 'collect',\n",
       " 'clippings',\n",
       " 'simple',\n",
       " 'familiar',\n",
       " 'latter',\n",
       " 'newly',\n",
       " 'wrote',\n",
       " 'out',\n",
       " 'marks',\n",
       " 'opened',\n",
       " 'critical',\n",
       " 'produce',\n",
       " 'shared',\n",
       " 'started',\n",
       " 'impression',\n",
       " 'industry',\n",
       " '__________________________________',\n",
       " 'view',\n",
       " 'preserved',\n",
       " 'breaking',\n",
       " 'buying',\n",
       " 'arms',\n",
       " 'small',\n",
       " 'into',\n",
       " 'nail',\n",
       " 'fortune',\n",
       " 'fold',\n",
       " 'columns',\n",
       " 'basic',\n",
       " 'originated',\n",
       " 'handed',\n",
       " 'blog',\n",
       " 'tips',\n",
       " 'web',\n",
       " 'seller',\n",
       " 'lines',\n",
       " 'look',\n",
       " 'mentioned',\n",
       " 'patch',\n",
       " 'soon',\n",
       " 'office',\n",
       " 'worked',\n",
       " 'sign',\n",
       " 'add',\n",
       " 'limited',\n",
       " 'shoe',\n",
       " 'empty',\n",
       " 'describe',\n",
       " 'circles',\n",
       " 'tapping',\n",
       " 'has',\n",
       " 'log',\n",
       " 'color',\n",
       " '_',\n",
       " 'yet',\n",
       " 'traces',\n",
       " 'display',\n",
       " 'sort',\n",
       " 'guide',\n",
       " 'much',\n",
       " 'actual',\n",
       " 'claims',\n",
       " 'only',\n",
       " 'detail',\n",
       " 'combining',\n",
       " 'nothing',\n",
       " 'edited',\n",
       " 'mint',\n",
       " 'commerce',\n",
       " 'inside',\n",
       " 'updated',\n",
       " 'recalled',\n",
       " 'column',\n",
       " 'afterward',\n",
       " 'both',\n",
       " 'then',\n",
       " 'broken',\n",
       " 'editions',\n",
       " 'touches',\n",
       " 'company',\n",
       " 'fair',\n",
       " 'others',\n",
       " 'answer',\n",
       " 'fill',\n",
       " 'shed',\n",
       " 'brown',\n",
       " 'drop',\n",
       " 'processed',\n",
       " 'mixes',\n",
       " 'producing',\n",
       " 'each',\n",
       " 'noticed',\n",
       " 'designs',\n",
       " 'upside',\n",
       " 'fabric',\n",
       " 'textbook',\n",
       " 'citing',\n",
       " 'claiming',\n",
       " 'promptly',\n",
       " 'logo',\n",
       " 'expanded',\n",
       " 'concluded',\n",
       " 'ironically',\n",
       " 'phrase',\n",
       " 'hard',\n",
       " 're',\n",
       " 'closing',\n",
       " 'ideas',\n",
       " 'linen',\n",
       " 'variously',\n",
       " 'meaning',\n",
       " 'in',\n",
       " 'virtually',\n",
       " 'talking',\n",
       " 'sheds',\n",
       " 'make',\n",
       " 'responded',\n",
       " 'having',\n",
       " 'finding',\n",
       " 'based',\n",
       " 'include',\n",
       " 'glowing',\n",
       " 'bottom',\n",
       " 'racks',\n",
       " 'distributing',\n",
       " 'complete',\n",
       " 'alone',\n",
       " 'reflecting',\n",
       " 'like',\n",
       " 'drops',\n",
       " 'simultaneously',\n",
       " 'presentation',\n",
       " 'an',\n",
       " 'red',\n",
       " 'proudly',\n",
       " 'sometimes',\n",
       " 'modeled',\n",
       " 'sample',\n",
       " 'classified',\n",
       " 'some',\n",
       " 'laced',\n",
       " 'publisher',\n",
       " 'reinforced',\n",
       " 'relies',\n",
       " 'painted',\n",
       " 'focus',\n",
       " 'whereas',\n",
       " 'ostensibly',\n",
       " 'contrary',\n",
       " 'tops',\n",
       " 'of',\n",
       " 'circulation',\n",
       " 'dropped',\n",
       " 'tap',\n",
       " 'alternatively',\n",
       " 'piles',\n",
       " 'working',\n",
       " 'passed',\n",
       " 'stored',\n",
       " 'scratch',\n",
       " 'blasting',\n",
       " 'op',\n",
       " 'thrown',\n",
       " 'brand',\n",
       " 'appear',\n",
       " 'paints',\n",
       " 'public',\n",
       " 'pencils',\n",
       " 'inserted',\n",
       " 'machinery',\n",
       " 'floating',\n",
       " 'pushed',\n",
       " 'discovered',\n",
       " 'such',\n",
       " 'incorporated',\n",
       " 'all',\n",
       " 'typically',\n",
       " 'gone',\n",
       " 'addresses',\n",
       " 'news',\n",
       " 'anyway',\n",
       " 'recommended',\n",
       " 'handful',\n",
       " 'ones',\n",
       " 'splashed',\n",
       " 'altogether',\n",
       " 'binder',\n",
       " 'daily',\n",
       " 'no',\n",
       " 'sorts',\n",
       " 'collections',\n",
       " 'resembles',\n",
       " 'reviewing',\n",
       " 'commenting',\n",
       " 'often',\n",
       " 'mere',\n",
       " 'thanks',\n",
       " 'leaving',\n",
       " 'repeating',\n",
       " 'preferred',\n",
       " 'examples',\n",
       " 'ads',\n",
       " 'thick',\n",
       " 'bringing',\n",
       " 'dubbed',\n",
       " 'e.g.',\n",
       " 'spelled',\n",
       " 'close',\n",
       " 'so',\n",
       " 'portions',\n",
       " 'free',\n",
       " 'instant',\n",
       " 'variety',\n",
       " 'commercial',\n",
       " 'stuff',\n",
       " 'adapted',\n",
       " 'lots',\n",
       " 'correspondence',\n",
       " 'exposed',\n",
       " 'standing',\n",
       " 'clearing',\n",
       " 'stripping',\n",
       " 'embossed',\n",
       " 'balance',\n",
       " 'cans',\n",
       " 'written',\n",
       " 'batch',\n",
       " 'creation',\n",
       " 'otherwise',\n",
       " 'recommends',\n",
       " 'being',\n",
       " 'little',\n",
       " 'did',\n",
       " 'includes',\n",
       " 'filtered',\n",
       " 'postal',\n",
       " 'translation',\n",
       " 'incorporate',\n",
       " 'follow',\n",
       " 'received',\n",
       " 'publishers',\n",
       " 'pattern',\n",
       " 'notably',\n",
       " 'uncovered',\n",
       " 'compares',\n",
       " 'conjunction',\n",
       " 'rag',\n",
       " 'translating',\n",
       " 'confidential',\n",
       " 'there',\n",
       " 'reflect',\n",
       " 'sell',\n",
       " 'envelopes',\n",
       " 'examined',\n",
       " 'pushing',\n",
       " 'forms',\n",
       " 'manufacturers',\n",
       " 'hammer',\n",
       " 'section',\n",
       " 'reams',\n",
       " 'analysis',\n",
       " 'poured',\n",
       " 'occasionally',\n",
       " 'stripped',\n",
       " 'together',\n",
       " 'analyzed',\n",
       " 'asking',\n",
       " 'hold',\n",
       " 'confirming',\n",
       " 'viewed',\n",
       " 'completely',\n",
       " 'supported',\n",
       " 'appearing',\n",
       " 'store',\n",
       " 'brochure',\n",
       " 'clearly',\n",
       " 'illustrations',\n",
       " 'pulled',\n",
       " 'emerged',\n",
       " 'editors',\n",
       " 'raising',\n",
       " 'create',\n",
       " 'recycling',\n",
       " 'distorted',\n",
       " 'shop',\n",
       " 'assembled',\n",
       " 'whose',\n",
       " 'began',\n",
       " 'lay',\n",
       " 'judging',\n",
       " 'caption',\n",
       " 'purposes',\n",
       " 'shell',\n",
       " 'food',\n",
       " 'part',\n",
       " 'archives',\n",
       " 'none',\n",
       " 'fall',\n",
       " 'memorandum',\n",
       " 'was',\n",
       " 'consumer',\n",
       " 'identical',\n",
       " 'cleaning',\n",
       " 'arguing',\n",
       " 'photographs',\n",
       " 'glossy',\n",
       " 'purchase',\n",
       " 'photographic',\n",
       " '‚Äî',\n",
       " 'why',\n",
       " 'goes',\n",
       " 'thought',\n",
       " 'wraps',\n",
       " 'deliberately',\n",
       " 'offering',\n",
       " 'reveals',\n",
       " 'bar',\n",
       " 'sensitive',\n",
       " 'blue',\n",
       " 'realized',\n",
       " 'auction',\n",
       " 'rest',\n",
       " 'opinion',\n",
       " 'time',\n",
       " 'buys',\n",
       " 'dated',\n",
       " 'corrected',\n",
       " 'explains',\n",
       " 'translate',\n",
       " 'glue',\n",
       " 'line',\n",
       " 'yes',\n",
       " 'mentioning',\n",
       " 'light',\n",
       " 'hot',\n",
       " 'inadvertently',\n",
       " 'nails',\n",
       " 'specialty',\n",
       " 'bits',\n",
       " 'ago',\n",
       " 'mind',\n",
       " 'suggestions',\n",
       " 'easily',\n",
       " 'chalk',\n",
       " 'perhaps',\n",
       " 'voluminous',\n",
       " 'crafted',\n",
       " 'discussing',\n",
       " 'produces',\n",
       " 'distributes',\n",
       " 'any',\n",
       " 'analyzing',\n",
       " 'instructions',\n",
       " 'newsletter',\n",
       " 'rarely',\n",
       " 'asserting',\n",
       " 'says',\n",
       " 'official',\n",
       " 'exposing',\n",
       " 'board',\n",
       " 'hers',\n",
       " 'abandoned',\n",
       " 'argued',\n",
       " 'review',\n",
       " 'lifted',\n",
       " 'translates',\n",
       " 'sleeve',\n",
       " 'replacing',\n",
       " 'regarding',\n",
       " 'naming',\n",
       " 'maintains',\n",
       " 'checked',\n",
       " 'suit',\n",
       " 'contends',\n",
       " 'privately',\n",
       " 'references',\n",
       " 'sized',\n",
       " 'taps',\n",
       " 'charging',\n",
       " 'including',\n",
       " 'reason',\n",
       " 'picks',\n",
       " 'flip',\n",
       " 'asserted',\n",
       " 'commentary',\n",
       " 'tapped',\n",
       " 'outline',\n",
       " 'sure',\n",
       " 'data',\n",
       " 'heads',\n",
       " 'properly',\n",
       " 'down',\n",
       " 'labelled',\n",
       " 'mug',\n",
       " 'considering',\n",
       " 'milk',\n",
       " 'exactly',\n",
       " 'learned',\n",
       " 'contains',\n",
       " 'site',\n",
       " 'unusual',\n",
       " 'practically',\n",
       " 'ubiquitous',\n",
       " 'seeing',\n",
       " 'outlining',\n",
       " 'tools',\n",
       " 'design',\n",
       " 'hence',\n",
       " 'stacked',\n",
       " 'story',\n",
       " 'door',\n",
       " 'deliver',\n",
       " 'probably',\n",
       " 'question',\n",
       " 'mailing',\n",
       " 'waste',\n",
       " 'marked',\n",
       " 'goods',\n",
       " ...]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(find_closest_embeddings(embedding_matrix[\"paper\"])[1:6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Encoding the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y_train to one hot vectors so that cross-entropy loss can be used\n",
    "y_train = to_categorical(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test)\n",
    "y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "NBmodel = MultinomialNB()\n",
    "# fit the model\n",
    "NBmodel.fit(X_train_emb, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_y_pred = NBmodel.predict(X_test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# use displot for newer versions\n",
    "ax1 = sns.distplot(y_test, hist=False, color=\"r\", label=\"Actual Values\")\n",
    "sns.distplot(NB_y_pred, hist=False, color=\"b\", label=\"Predicted Values\" , ax=ax1)\n",
    "\n",
    "\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different dropouts\n",
    "model.add(LSTM(units = 256, return_sequences=True, input_shape = (168,50)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "model.add(Dense(units=20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different optimizers\n",
    "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different validation split\n",
    "# try different epochs\n",
    "res = model.fit(X_train_emb, y_train, validation_split=0.2, batch_size=32, epochs=100, verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Model performance overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Confusion matrix and correlation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare my results to other models?\n",
    "- https://www.kaggle.com/code/satwiksrivastava/emoji-prediction/notebook\n",
    "- https://huggingface.co/spaces/ml6team/emoji_predictor\n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes rule\n",
    "1. vector len 6 {freq of each emotion var t}\n",
    "2. vector len 25 {freq of each emoji var j}\n",
    "3. matrix size 25x6 {keys: j, values: t } \n",
    "    and j,t entry has freq of emotion t given emoji j\n",
    "### Use bayes to create another table\n",
    "4. matrix 6x25 {keys: t, values: j} \n",
    "    and t,j entry is prob of emoji j given emotion t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('capstone-proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11b89a8208dc62c4b368a2fc84cebd8a1aafdd7a9e7b8f254ebc1eaaea760d8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
